## 阶段一环境配置

### 前置准备

```
1.修改主机名
	1.vi /etc/hostname
	2.hostnamectl set-hostname 主机名
	3.ctrl + D
	
2.配置hosts
	1.vi /etc/hosts
	ip+master
	ip+slave1
	ip+slave2
	
3.关闭防火墙
	1.查看防火墙状态
		systemctl status firewalld
	2.关闭防火墙
		systemctl stop firewalld
	3.开启开机自动关闭防火墙
		systemctl disable firewalld
	4.配置 /etc/selinux/confg 
		enconfig = disabled
		
4.配置ssh
	1.生成公钥
		ssh-keygen -t rsa
	2.copy公钥到其他节点
		ssh-copy-d master /slave1/slave2
	3.测试免密是否成功
		ssh master/slave1/slave2

5.查看是否自带jdk
	rpm -qa | grep java
```



### 一、配置JDK

```
1. 环境变量
export JAVA_HOME=/usr/local/src/jdk
export PATH=$PATH:$JAVA_HOME/bin

2.测试版本
java  -version
javac -version

3.刷新配置文件
source /root/base_profile
```



### 二、配置HADOOP

```xml
1.环境变量
export HADOOP_HOME=/usr/local/src/hadoop
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

2.配置core-site.xml
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
</property>

<property>
    <name>hadoop.tmp.dir</name>
    <value>/opt/data/hadoop</value>
</property>

3.配置dfs-site.xml
<property>
    <name>hdfs.replication</name>
    <value>3</value>
</property>

4.配置mapduce-site.xml
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>

5.配置yarn-site.xml
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
</property>

<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>

<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>

<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>

6.配置workes
master
slave1
salve2

7.copy hadoop 到从节点
 scp -r /usr/local/src/hadoop slave1:/usr/local/src
 scp -r /usr/local/src/hadoop slave2:/usr/local/src

8.格式化hadoop
 hdfs namenode -format

9.jps 查看节点
DATANODE
NAMENODE
SECONDARYNAMENODE
resourcemanager
nodemanager
```



### 三、配置SPARK

```
1.配置环境变量
export SPARK_HOME=/usr/local/src/spark
export PATH=$PATH:$SPARK_HOME/sbin:$SPARK_HOME/bin

2.刷新soure
source /root/bash_profile

3.配置spark.env.sh
export JAVA_HOME=/usr/local/src/jdk
export HADOOP_CONF_DIR=/usr/local/src/hadoop/etc/hadoop
export SPARK_CONF_DIR=/usr/local/src/spark/conf
export SPARK_MASTER_HOST=master/local/127.0.0.1
export SPARK_MASTER_PORT=7077

4.配置workes
master
slave1
slave2

5.copy spark 到从节点
 scp -r /usr/local/src/hadoop slave1:/usr/local/src
 scp -r /usr/local/src/hadoop slave2:/usr/local/src

6.启动spark 案例
 集群的方式
 spark-submit --class org.apache.spark.example.SparkPi jar…… 100

```



### 四、配置FLINK -- 需要先关闭spark 端口占用

```
1. 配置环境变量
export FLINK_HOME=/usr/local/src/flink
export PATH=$PATH:$FLINK_HOME/bin

2.配置flink.conf.yaml
address: master 

3.配置workers
master
slave1
slave2

4.copy spark 到从节点
 scp -r /usr/local/src/hadoop slave1:/usr/local/src
 scp -r /usr/local/src/hadoop slave2:/usr/local/src
 
5.启动 flink 集群案例
flink run WordCount.jar 
```



### 五、配置HIVE

```xml
1. 配置环境变量
export HIVE_HOME=/usr/local/src/hive
export PATH=$PATH:$HIVE_HOME/bin

2.配置 hive-site.xml
1. touch hive-site.xml  然后把 hive-default.xml.template 的标头copy 过来

<property>
	<name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
</property>

<property>
	<name>javax.jdo.option.ConnectionPassword</name>
    <value>123456</value>
</property>

<property>
	<name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false&amp;characterEncoding=UTF-8</value>
</property>

<property>
	<name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
</property>

2. 删除 log4h-slf4j.jar
rm -rf log4j-slf4j-impl-2.10.0.jar

3.删除guava
rm -rf guava

4.copy hadoop 的guava 到hive的lib 目录下
cp /usr/local/src/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar  /usr/local/src/hive/lib/

5.copy mysql5.7 jar 到lib 目录下

6.初始化hive元数据
schematool -dbType mysql --Schema

```



### 六、配置ZOOKEEPER

```
1.配置环境变量
export ZOOKEEPER_HOME=/usr/local/src/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin

2.配置 zoo.cfg
cp  zoo_sample.cfg zoo.cfg

dataDir=/usr/local/src/zookeeper/data
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888

3.配置 /data/myid
在 conf 同级别目录下 新建 data / myid 
master : 1
slave1 : 2
slave2 : 3

4.zookeeper 分发到从节点
scp -r /usr/local/src/zookeeper slave1:/usr/local/src/
scp -r /usr/local/src/zookeeper slave2:/usr/local/src/

5.启动zookeeper 并且查看状态
zkServer.sh start  启动
zkServer.sh status 查看状态
zkServer.sh stoop 关闭zookeeper
```



### 七、配置KAFKA -- 阶段四重点

```
1. 配置环境变量
export KAFKA_HOME=/usr/local/src/kafka
export PATH=$PATH:$=KAFAK_HOME/bin

2.配置server.property
broker.id = 1
hostname=master
listeners = PLAINTEXT://0.0.0.0:9092
advertised.host.name=192.168.43.191
advertised.listeners=PLAINTEXT://192.168.43.191:9092

logs
logs=/usr/local/src/kafka/logs

zookeeper
master:2181,salve1:2181,salve2:2181


1.查看kafka主题
	kafka-topic.sh --list --zookeeper master:2181
2.创建kafka主题
	kafka-topic.sh --create --zookeeper maser:2181 --replication-factor 1 --partitions --topic zhang
3.开启生产者 -- produce
	kafka-console.produce.sh --broker-list master:9092 --topic zhang
4.开启消费者 -- consumer
	kafka-console.consumer.sh bootstrap-server maste:9092 --form-beginning 

```



###  八、配置FLUME

```
1.配置环境变量
export FLUME_HOME=/usr/local/src/flume
export PATH=$PATH:$FLUME_HOME/bin

2. 配置数据 来自端口 打印到控制台
    vi duankou.conf
    a1.sources=r1
    a1.channels=c1
    a1.sinks=k1

    a1.sources.r1.type=netcat
    a1.sources.r1.bind=master
    a1.sources.r1.port=12000

    a1.channels.c1.type=memory

    a1.sinks.k1.type=logger

    a1.sources.r1.channels=c1
    a1.sinks.k1.channel=c1
    
 结果打印到kafka
 a1.sources=r1
 a1.channels=c1
 a1.sinks=k1
 
 a1.sources.k1.type=netcat
 a1.sources.k1.bind=master
 a1.sources.k1.port=12000
 
 a1.channels.c1.type=memory
 
 数据打印到kafka
 a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink
 a1.sinks.k1.brokerList=mster:9092,slave1:9092,slave2:9092
 a1.sinks.k1.topic=zhang
 
 a1.sources.r1.channels=c1
 a1.sinks.k1.channel=c1
 
 启动flume
flume-ng agent -c conf -f /usr/local/src/flume/conf/文件名.conf --name a1 -Dflume.root.logger=INFO,console
```





