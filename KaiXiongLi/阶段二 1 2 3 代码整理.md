## 阶段二、前三小题代码整理

##### 一、前置环境准备

```
1.集群环境
	JDK | HADOOP | ZOOKEEPER | KAFKA | FLUME | HBASE | MAXWELL 
	
2.kafak主题
	(1). kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic ods_mall_data --from-beginning
	(2). kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic ods_mall_log --from-beginning
	(3). kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic fact_order_master --from-beginning
	(4). kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic fact_order_detail --from-beginning
	(5). kafka-console-consumer.sh --bootstrap-server bigdata1:9092 --topic dim_customer_login_log --from-beginning
	
3.flume采集
	(1).bind:localhost
	(2).port:25001
	(3).topic:ods_mall_log

4.maxwell采集
	(1).my.cnf --> ds_realtime_db
	(2).kafka_topic=ods_mall_data
	
5.hbase 
	create_namespace "dsj","info"
	
```



##### 二、题目解析

```
第一题:
在Master节点使用Flume采集实时数据生成器25001端口的socket数据（实时数据生成器脚本为Master节点/data log目录下的gen ds data to socket脚本，该脚本为Master节点本地部署且使用socket传输)，将数据存入到Kafka的Topic中(Topic名称为ods mall log,分区数为4)，使用Kafka自带的消费者消费ods mal卫
(Topic)中的数据，查看前2条数据的结果:

(1):在Master节点使用Flume采集实时数据生成器25001端口的socket数据
	配置flume端口为:25001
	
(2):将数据存入到Kafka的Topic中(Topic名称为ods mall log,分区数为4)
	1.创建topic:ods_mall_log partitons = 4
	2.数据到kafak的主题为:ods_mall_log
	
(3):使用Kafka自带的消费者消费ods mal卫(Topic)中的数据，查看前2条数据的结果:
	1.启动kafka消费者 消费主题:ods_mall_log 并且只查看前两天数据：--max-messages 2
	2.kafka-console.consumer.sh --bootstrap-server bigdata1:9092 --topoic ods_mall_log --from-beginning --max-messages 2

```



```
第二题:
	在Master节点进入到maxwel1-1.29.0的解压后目录下（在/opt/module),配置相关文件并启动，读取MySQL数据的binlog日志(mysql的binlog相关配置已完毕)到Kafka的Topic中(Topic名称为ods mall data,分区数为4)。使用Kafka自带的消费者消费ods mall data(Topic)中的数据，查看前2条数据的结果
	
(1):读取MySQL数据的binlog日志
	binlog=_format=row -- 默认是配置完毕
	
(2):到Kafka的Topic中(Topic名称为ods mall data,分区数为4)
	1.创建topic:ods_mall_data partitons = 4
	2.数据到kafak的主题为:ods_mall_data  -->kafak_topic=ods_mall_data
	
(3):使用Kafka自带的消费者消费ods mall data(Topic)中的数据，查看前2条数据的结果
	1.启动kafka消费者 消费主题:ods_mall_data 并且只查看前两天数据：--max-messages 2
	2.kafka-console.consumer.sh --bootstrap-server bigdata1:9092 --topoic ods_mall_data --from-beginning --max-messages 2
```





```scala
第三题:
使用Flink消费Kafka 中 topic为ods mall data的数据，根据数据中不同的表将数据分别分发至kafka...的 DWD层的 fact_order_master .fact_order..detail的Topic中(只获取data的内容，具体的内容格式考生请自查），其他的表则无需处理;。

(1).使用Flink消费Kafka 中 topic为ods mall data的数据
	数据源:ods_mall_data 
	
(2).根据数据中不同的表将数据分别分发至kafka的dwd层:fact_order_master | fact_order_detail 主题中
	需要把获取到的数据分流处理 
(3).只获取data的内容
	端口发送的数据很多很杂：需要过滤出有用的数据 --> data
//代码实现
    package com.ynnz.FlinkKafka.Demo1.Demo2

import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.ProcessFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.connectors.kafka.{FlinkKafkaConsumer, FlinkKafkaProducer}
import org.apache.flink.util.Collector
import org.apache.hbase.thirdparty.com.google.gson.JsonParser

import java.util.Properties

/**
 * Author:李开雄
 * 时  间: 2023/9/4
 * class: ${class_name}
 */
object KafkaOdsToKafDwd {
  def main(args: Array[String]): Unit = {
    //设置流的执行环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    //设置并行度为1
    env.setParallelism(1)
    //设置处理时间
    env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)

    // TODO 设置kafka数据源↓
    val prop = new Properties();
    prop.setProperty("bootstrap.servers","192.168.43.191:9092,192.168.43.192:9092,192.168.43.193:9092")
    prop.setProperty("group.id","liKaiHusIng")

    //flink对接kafka中的ods_mall_data
    //addSource:添加数据来源 | 主题 | 数据格式 | kafka地址
    // FlinkKafkaConsumer[String] | 消费者  | String == 数据类型
    // TODO 添加数据源↓
    val dataSource = env.addSource(new FlinkKafkaConsumer[String]("ods_mall_data",new SimpleStringSchema(),prop))

    // TODO 数据分流 -- 分流到两个主题中 fact_order_master | fact_order_detail
    val out1 = new OutputTag[String]("order")
    val out2 = new OutputTag[String]("detail")

    val stream = dataSource.process(new ProcessFunction[String,String] {
      override def processElement(i: String, context: ProcessFunction[String, String]#Context, collector: Collector[String]): Unit = {
        //TODO i ==> 每条数据 ==> json 格式 ==> 转成对象进行操作
        val jsonObj = new JsonParser().parse(i).getAsJsonObject
        val tableName = jsonObj.get("table").getAsString
        //TODO data数据
        val data = jsonObj.get("data").getAsJsonObject.toString
        if(tableName.equals("order_master")){
          context.output(out1,data)
        }else if(tableName.equals("order_detail")){
          context.output(out2,data)
        }else{
          collector.collect(i)
        }
      }
    })

    //取出分流的数据 并存入 kafka对应的主题中 | fact_order_master | fact_order_detail
    val order_stream  = stream.getSideOutput(out1)
    val detail_stream = stream.getSideOutput(out2)
    order_stream.addSink( new FlinkKafkaProducer[String]("fact_order_master",new SimpleStringSchema(),prop))
    detail_stream.addSink(new FlinkKafkaProducer[String]("fact_order_detail",new SimpleStringSchema(),prop))

    //备份到hbase
    order_stream.addSink(new KafkaToHbase("dsj:order_master"))
    detail_stream.addSink(new KafkaToHbase("dsj:order_detail"))

        //关闭执行环境
    env.execute("KafkaOdsToKafakDwd")
  }

}
                                                                                                                      
                                                                                                                      
  //连接ZOOKEEPER的端口方法  以及 数据存入hbase的实现
  // TODO 获取数据库连接 | ZOOKEEPER 地址 | ZOOKEEPER 端口 | HBaseConfiguration | HConstants | ConnectionFactory |
	package com.ynnz.FlinkKafka.Demo1.Demo2

import com.ibm.icu.text.SimpleDateFormat
import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}
import org.apache.hadoop.hbase.{HBaseConfiguration, HConstants, TableName}
import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory, Put}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hbase.thirdparty.com.google.gson.JsonParser

import java.util.Date
import scala.util.Random

/**
 * Author:李开雄
 * 时  间: 2023/9/4
 * class: ${class_name}
 */
class KafkaToHbase(val tbName:String) extends RichSinkFunction[String]{
  // TODO 获取数据库连接 | ZOOKEEPER 地址 | ZOOKEEPER 端口 | HBaseConfiguration | HConstants | ConnectionFactory |
  def getHbaseConn():Connection = {
    val config = HBaseConfiguration.create()
    config.set(HConstants.ZOOKEEPER_QUORUM,"192.168.43.191:2181,192.168.43.192:2181,192.168.43.193:2181")
    config.set(HConstants.ZOOKEEPER_CLIENT_PORT,"2181")
    val conn = ConnectionFactory.createConnection(config)
    conn
  }
  lazy val conn  = getHbaseConn()
  lazy val table  = conn.getTable(TableName.valueOf(tbName))
  //日期格式化 | row_key
  val sdf = new SimpleDateFormat("yyyyMMddHHmmssSSS")

  //TODO 将数据写入hbase对应的表中
  override def invoke(value: String, context: SinkFunction.Context): Unit = {
    //TODO rowKey | info
    val rowKey = Random.nextInt(10)+sdf.format(new Date())

    // put 把内容加入hbase中
    val put = new Put(Bytes.toBytes(rowKey))

    //TODO 获取数据中的 key 和 value
   val jsonObj = new JsonParser().parse(value).getAsJsonObject
   val  iterator = jsonObj.entrySet().iterator()
    while(iterator.hasNext){
      val entity = iterator.next() //TODO 获取数据对 key - value
      val key   = entity.getKey
      val value = entity.getValue.getAsString
      put.addColumn(Bytes.toBytes("info"),Bytes.toBytes(key),Bytes.toBytes(value))
    }
    //数据写入hbase
    table.put(put)





  }

  override def close(): Unit = {
    table.close()
    conn.close()
  }
}                                                                                                                                                                   
```

##### 三、结果展示

![1](E:\git-development\bigdata\90NationalCompetition\KaiXiongLi\1.png)



![2](E:\git-development\bigdata\90NationalCompetition\KaiXiongLi\2.png)

![3](E:\git-development\bigdata\90NationalCompetition\KaiXiongLi\3.png)